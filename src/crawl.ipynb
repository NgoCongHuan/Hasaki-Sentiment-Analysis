{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from unidecode import unidecode\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_all_categories():\n",
    "\n",
    "    response = requests.get(\"https://hasaki.vn/\")\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    json_categories = []\n",
    "\n",
    "    for category_link in soup.find_all('a', class_='text_dmuc'):\n",
    "        \n",
    "        category = {}\n",
    "        \n",
    "        category['Tên danh mục'] = category_link.text\n",
    "        \n",
    "        category['Liên kết'] = category_link.attrs['href']\n",
    "\n",
    "        category['Mã danh mục'] = category['Liên kết'].replace('.html','').split('-')[-1].replace('c','')\n",
    "        \n",
    "        json_categories.append(category)\n",
    "\n",
    "    df_categories = pd.json_normalize(json_categories)\n",
    "\n",
    "    return df_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_comments_selenium(product_link, product_id):\n",
    "    \n",
    "    # Initialize the WebDriver (you might need to specify the path to the ChromeDriver executable)\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    # Open the web page\n",
    "    driver.get(product_link)\n",
    "\n",
    "    # Wait until the pagination is present\n",
    "    pagination = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"pagination_comment\")))\n",
    "\n",
    "    # Get the number of pages from the data-max attribute of the next button\n",
    "    next_button = driver.find_element(By.CSS_SELECTOR, \".item_next_sort\")\n",
    "    max_page = int(next_button.get_attribute(\"data-max\"))\n",
    "\n",
    "    comments_of_product = []\n",
    "\n",
    "    for page in range(1, max_page + 1):\n",
    "        \n",
    "        try:\n",
    "            # Locate the page link by its rel attribute\n",
    "            page_link = driver.find_element(By.CSS_SELECTOR, f\"a[rel='{page}']\")\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView(true);\", page_link)\n",
    "            driver.execute_script(\"arguments[0].click();\", page_link)  # Use JavaScript to click the link\n",
    "            \n",
    "            # Wait for the page to load comments (adjust the wait time as needed)\n",
    "            time.sleep(2)\n",
    "\n",
    "            comments = driver.find_elements(By.CLASS_NAME, 'item_comment')\n",
    "\n",
    "            for comment in comments:\n",
    "\n",
    "                dict_comment = {}\n",
    "\n",
    "                comment_html = comment.get_attribute('outerHTML')\n",
    "                \n",
    "                soup = BeautifulSoup(comment_html, 'html.parser')\n",
    "\n",
    "                dict_comment['Mã sản phẩm'] = product_id\n",
    "\n",
    "                dict_comment['Ngày'] = soup.find('div', class_='timer_comment').text\n",
    "\n",
    "                dict_comment['Tên khách hàng'] = soup.find('div', class_='title_comment').text.strip()\n",
    "\n",
    "                dict_comment['Số sao đánh giá'] = int(soup.find('div', class_='number_start').attrs['style'].split(':')[1].replace('%', '').replace(';',''))/20\n",
    "\n",
    "                dict_comment['Nội dung đánh giá'] = soup.find('div', class_='content_comment').text\n",
    "\n",
    "                comments_of_product.append(dict_comment)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error clicking page {page}: {e}\")\n",
    "\n",
    "    # Close the driver\n",
    "    driver.quit()\n",
    "\n",
    "    return comments_of_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_product(product_link, product_sold, category_id):\n",
    "    \n",
    "    reponse = requests.get(product_link)\n",
    "\n",
    "    soup = BeautifulSoup(reponse.content, 'html.parser')\n",
    "\n",
    "    product = {}\n",
    "\n",
    "    product['Liên kết'] = product_link\n",
    "\n",
    "    product['Ảnh sản phẩm'] = soup.find('img', id='zoom_01').attrs['src']\n",
    "\n",
    "    product_id = soup.find('span', class_='item-sku').text.split(':')\n",
    "    \n",
    "    product['Mã sản phẩm'] = product_id[1].strip()\n",
    "\n",
    "    product['Tên sản phẩm'] = soup.find('span', class_='product__title').text\n",
    "\n",
    "    product['Mã danh mục'] = category_id\n",
    "\n",
    "    product['Số sản phẩm đã bán'] = product_sold\n",
    "\n",
    "    product['Giá'] = soup.find('span', id='product-final_price').text.replace('\\n', '').strip()\n",
    "\n",
    "    hasaki_price_info = soup.find('div', class_='hasaki-price-info')\n",
    "\n",
    "    if hasaki_price_info:\n",
    "\n",
    "        product['Giá thị trường'] = soup.find('span', id='market_price').text\n",
    "\n",
    "        product['Tiết kiệm'] = soup.find('span', id='save_money').text\n",
    "\n",
    "        product['Phần trăm giảm'] = soup.find('span', id='save_money_percent').text\n",
    "\n",
    "    product['Đánh giá trung bình'] = soup.find('div', class_='txt_numer').text\n",
    "\n",
    "    product['Số lượt đánh giá'] = soup.find('div', class_='txt_total_nhanxet').text.split(' ')[0]\n",
    "\n",
    "    product['Số lượt hỏi đáp'] = soup.find('a', id='click_scroll_qa').text.strip(' ').split(' ')[0]\n",
    "\n",
    "    tb_info_sanpham = soup.find('table', class_='tb_info_sanpham').find_all('tr')\n",
    "\n",
    "    for info in tb_info_sanpham:\n",
    "        \n",
    "        td = info.find_all('td')\n",
    "        \n",
    "        info_name = td[0].text\n",
    "        \n",
    "        info_content = td[1].text\n",
    "        \n",
    "        product[info_name] = info_content\n",
    "\n",
    "    box_thanhphanchinh = soup.find('div', id='box_thanhphanchinh').find('div', class_='ct_box_detail')\n",
    "    if box_thanhphanchinh is not None:\n",
    "        product['Thành phần sản phẩm'] = box_thanhphanchinh.text\n",
    "    else:\n",
    "        product['Thành phần sản phẩm'] = 'Không có thông tin'\n",
    "\n",
    "    box_huongdansudung = soup.find('div', id='box_huongdansudung').find('div', class_='ct_box_detail')\n",
    "    if box_huongdansudung is not None:\n",
    "        product['Hướng dẫn sử dụng'] = box_huongdansudung.text\n",
    "    else:\n",
    "        product['Hướng dẫn sử dụng'] = 'Không có thông tin'\n",
    "\n",
    "    if (int(product['Số lượt đánh giá']) > 0) and (int(product['Số lượt đánh giá'])) <= 10 :\n",
    "        \n",
    "        comments = soup.find_all('div', class_='item_comment')\n",
    "\n",
    "        comments_of_product = []\n",
    "\n",
    "        for comment in comments:\n",
    "\n",
    "            dict_comment = {}\n",
    "\n",
    "            dict_comment['Mã sản phẩm'] = product['Mã sản phẩm']\n",
    "\n",
    "            dict_comment['Ngày'] = comment.find('div', class_='timer_comment').text\n",
    "\n",
    "            dict_comment['Tên khách hàng'] = comment.find('div', class_='title_comment').text.strip()\n",
    "\n",
    "            dict_comment['Số sao đánh giá'] = int(comment.find('div', class_='number_start').attrs['style'].split(':')[1].replace('%', '').replace(';',''))/20\n",
    "\n",
    "            dict_comment['Nội dung đánh giá'] = comment.find('div', class_='content_comment').text\n",
    "\n",
    "            comments_of_product.append(dict_comment)\n",
    "\n",
    "        return product, comments_of_product\n",
    "    \n",
    "    if (int(product['Số lượt đánh giá']) > 10):\n",
    "        \n",
    "        comments_of_product = crawl_comments_selenium(product['Liên kết'], product['Mã sản phẩm'])\n",
    "\n",
    "        return product, comments_of_product\n",
    "    \n",
    "    return product, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_all_products_in_category(category_name, category_link, category_id):\n",
    "\n",
    "    page = 1\n",
    "\n",
    "    json_products = []\n",
    "\n",
    "    json_comments = []\n",
    "\n",
    "    total_products = 0\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        response = requests.get(f\"{category_link}?p={page}\")\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        products = soup.find_all('div', class_='ProductGridItem__itemOuter')\n",
    "\n",
    "        if len(products) == 0:\n",
    "            \n",
    "            sys.stdout.write(f\"\\nAll products in the '{category_name}' category have been successfully retrieved\\n\")\n",
    "            sys.stdout.write('\\n')\n",
    "            \n",
    "            break\n",
    "\n",
    "        else:\n",
    "            for product in products:\n",
    "                \n",
    "                product_link = product.find('a', class_='v3_thumb_common_sp').attrs['href']\n",
    "\n",
    "                item_count_by = product.find('span', class_='item_count_by')\n",
    "                if item_count_by is not None:\n",
    "                    product_sold = item_count_by.text.strip()\n",
    "                else:\n",
    "                    product_sold = 0\n",
    "\n",
    "                dict_product, comments_of_product = extract_product(product_link, product_sold, category_id)\n",
    "\n",
    "                json_products.append(dict_product)\n",
    "\n",
    "                if comments_of_product != False:\n",
    "                    \n",
    "                    json_comments.append(comments_of_product)\n",
    "\n",
    "                total_products+=1\n",
    "\n",
    "                sys.stdout.write(f'Category: {category_name} | Page : {page} | Total: {total_products}\\r')\n",
    "\n",
    "        page += 1\n",
    "\n",
    "    json_comments = sum(json_comments, [])\n",
    "\n",
    "    return json_products, json_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    df_categories = crawl_all_categories()\n",
    "    df_categories.to_csv('../data/raw/hasaki_categories.csv', encoding='utf-8-sig', index=False)\n",
    "\n",
    "    for index in range(0,df_categories.shape[0]):\n",
    "\n",
    "        category_name = df_categories['Tên danh mục'].loc[index]\n",
    "        category_link = df_categories['Liên kết'].loc[index]\n",
    "        category_id = df_categories['Mã danh mục'].loc[index]\n",
    "        \n",
    "        json_products, json_comments = crawl_all_products_in_category(category_name, category_link, category_id)\n",
    "\n",
    "        df_products = pd.json_normalize(json_products)\n",
    "\n",
    "        df_comments = pd.json_normalize(json_comments)\n",
    "\n",
    "        directory = unidecode(category_name.lower()).replace(' ','_')\n",
    "\n",
    "        directory_path = Path(f'../data/raw/{directory}')\n",
    "\n",
    "        directory_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        df_products.to_csv(directory_path / 'products.csv', encoding='utf-8-sig', index=False)\n",
    "        \n",
    "        df_comments.to_csv(directory_path / 'comments.csv', encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merged_csv(object):\n",
    "    \n",
    "    df_merged = pd.DataFrame()\n",
    "    \n",
    "    paths = [Path(f'../data/raw/{directory}/{object}.csv') for directory in os.listdir('../data/raw') if os.path.isdir(Path(f'../data/raw/{directory}'))]\n",
    "\n",
    "    for path in paths:\n",
    "        df = pd.read_csv(path)\n",
    "        df_merged = pd.concat([df_merged, df],ignore_index=True)\n",
    "\n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = ['products', 'comments']\n",
    "\n",
    "for object in objects:\n",
    "    df_merged = merged_csv(object)\n",
    "    df_merged.to_csv(f'../data/raw/hasaki_{object}.csv', encoding='utf-8-sig', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
